{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae08ba9-2862-4131-a0ed-309d3bafc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Beta\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10948-a930-4302-a120-e84778a3dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.CNN = nn.Sequential(nn.Conv2d(3, 8, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                 nn.BatchNorm2d(8),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(8, 8, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                 nn.BatchNorm2d(8),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(8, 16, kernel_size = 3, stride = 2, padding = 1),\n",
    "                                 nn.BatchNorm2d(16),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(16, 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                 nn.BatchNorm2d(16),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(16, 32, kernel_size = 3, stride = 2, padding = 1),\n",
    "                                 nn.BatchNorm2d(32),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(32, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                 nn.BatchNorm2d(32),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(32, 32, kernel_size = 1, stride = 1, padding = 1),\n",
    "                                 nn.BatchNorm2d(32),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.AdaptiveAvgPool2d((4,4)),\n",
    "                                 )\n",
    "\n",
    "        self.c_g = nn.Sequential(nn.Linear(10, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 32),\n",
    "                               )\n",
    "\n",
    "        nn.init.zeros_(self.c_g[4].weight)\n",
    "        nn.init.constant_(self.c_g[4].bias, 0)\n",
    "\n",
    "        self.s_g = nn.Sequential(nn.Linear(10, 16),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(16, 16),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(16, 16),\n",
    "                       )\n",
    "\n",
    "        nn.init.zeros_(self.s_g[4].weight)\n",
    "        nn.init.constant_(self.s_g[4].bias, 0)\n",
    "\n",
    "        self.FC = nn.Sequential(nn.Linear(32*16, 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.LayerNorm(128),\n",
    "                               )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _preprocess(self, x: np.array) -> (torch.tensor, torch.tensor):\n",
    "        # x: (n, 96, 96, 3) uint8 or float\n",
    "    \n",
    "        x = torch.from_numpy(x).float().to(self.device)\n",
    "    \n",
    "        x_map = x[:, :84, :, :]   #(n, 84, 96, 3)\n",
    "    \n",
    "        speed = torch.sum(x[:, 91:94, 12:15, :], dim = (1,2,3))/4317    #(n, )\n",
    "        ABS = torch.sum(x[:, 91:94, 17:27, :], dim = (1,2,3))/4851      #(n, )\n",
    "        left_steer = torch.sum(x[:, 86:92, 38:49, 1], dim = (1,2))/12563       #(n, )\n",
    "        right_steer = torch.sum(x[:, 86:92, 48:59, 1], dim = (1,2))/12543      #(n, )\n",
    "        left_gyro = torch.sum(x[:, 86:92, 58:73, 0], dim = (1,2))/14000        #(n, )\n",
    "        right_gyro = torch.sum(x[:, 86:92, 72:87, 0], dim = (1,2))/14000       #(n, )\n",
    "\n",
    "        x_gauge = torch.stack((speed, ABS, left_steer, right_steer, left_gyro, right_gyro, \n",
    "                               speed*left_steer, speed*right_steer,\n",
    "                              ABS*left_steer, ABS*right_steer), dim = 1)        #(n, 10)\n",
    "\n",
    "        x_map = x_map.permute(0, 3, 1, 2)      #(n, 3, 84, 96)\n",
    "    \n",
    "        return x_map, x_gauge\n",
    "\n",
    "    def forward(self, x: np.array) -> torch.tensor:\n",
    "\n",
    "        x_map, x_gauge = self._preprocess(x)\n",
    "        x_map = self.CNN(x_map)\n",
    "        \n",
    "        c_g = self.c_g(x_gauge)\n",
    "        c_g = 1 + 0.2*torch.tanh(c_g)\n",
    "        c_g = c_g.reshape(-1,32,1,1)\n",
    "\n",
    "        s_g = self.s_g(x_gauge)\n",
    "        s_g = 1 + 0.2*torch.tanh(s_g)\n",
    "        s_g = s_g.reshape(-1,1,4,4)\n",
    "\n",
    "        x = x_map*c_g*s_g\n",
    "\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "\n",
    "        x = self.FC(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Critic_Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        critic_layers = [\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        ]                         \n",
    "\n",
    "        self.critic = nn.Sequential(*critic_layers).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        state_values = self.critic(x).squeeze(-1)                \n",
    "        \n",
    "        return state_values\n",
    "\n",
    "class Actor_Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        actor_shared_layers = [\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        self.actor_shared = nn.Sequential(*actor_shared_layers)\n",
    "        self.actor_steers = nn.Sequential(nn.Linear(128, 2))\n",
    "        self.actor_accels = nn.Sequential(nn.Linear(128, 2))\n",
    "\n",
    "        nn.init.zeros_(self.actor_steers[0].weight)\n",
    "        nn.init.constant_(self.actor_steers[0].bias, -3)\n",
    "\n",
    "        nn.init.zeros_(self.actor_accels[0].weight)\n",
    "        nn.init.constant_(self.actor_accels[0].bias, -3)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        actions_shared = self.actor_shared(x)       #[n_evns, 32]\n",
    "\n",
    "        actions_steers_ab = self.actor_steers(actions_shared) # shape: [n_envs, 2]\n",
    "        actions_accels_ab = self.actor_accels(actions_shared) # shape: [n_envs, 2]\n",
    "\n",
    "        actions_steers_ab = 1 + 1e-3 + F.softplus(actions_steers_ab)\n",
    "        actions_accels_ab = 1 + 1e-3 + F.softplus(actions_accels_ab)\n",
    "                \n",
    "        return  actions_steers_ab, actions_accels_ab\n",
    "\n",
    "\n",
    "class PPO_Net(nn.Module):\n",
    "\n",
    "        def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.CNN_Net = CNN_Net(device)\n",
    "            self.Critic_Net = Critic_Net(device)\n",
    "            self.Actor_Net = Actor_Net(device)\n",
    "\n",
    "    \n",
    "class PPO():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        n_envs: int,\n",
    "    ):\n",
    "\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        self.PPO_Net = PPO_Net(device)\n",
    "        \n",
    "    def optimizer(self, critic_lr, actor_lr, cnn_lr) -> None:\n",
    "\n",
    "        self.PPO_optimizer = torch.optim.AdamW([{\"params\": self.PPO_Net.CNN_Net.parameters(), \"lr\":cnn_lr, \"weight_decay\": 0.0},\n",
    "                                              {\"params\": self.PPO_Net.Critic_Net.parameters(), \"lr\":critic_lr, \"weight_decay\":0.0},\n",
    "                                              {\"params\": self.PPO_Net.Actor_Net.parameters(), \"lr\":actor_lr, \"weight_decay\": 0.0}])\n",
    "\n",
    "        \n",
    "    def select_action(\n",
    "        self, x: np.ndarray\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        x = self.PPO_Net.CNN_Net(x)\n",
    "\n",
    "        state_values = self.PPO_Net.Critic_Net(x)\n",
    "        actions_steers_ab, actions_accels_ab = self.PPO_Net.Actor_Net(x)\n",
    "\n",
    "        c_actions_steers = actions_steers_ab[:,0] + actions_steers_ab[:,1]\n",
    "        c_actions_accels = actions_accels_ab[:,0] + actions_accels_ab[:,1]\n",
    "\n",
    "        regulation = F.relu(c_actions_steers - c_max).pow(2) + F.relu(c_actions_accels - c_max).pow(2)\n",
    "\n",
    "        steerings_dist = Beta(actions_steers_ab[:, 0], actions_steers_ab[:, 1])\n",
    "        accels_dist = Beta(actions_accels_ab[:, 0], actions_accels_ab[:, 1])\n",
    "\n",
    "        steerings = steerings_dist.rsample()\n",
    "        accels = accels_dist.rsample()\n",
    "        \n",
    "        # total log prob per environment\n",
    "        action_log_probs = (\n",
    "            steerings_dist.log_prob(steerings) + accels_dist.log_prob(accels)\n",
    "        )\n",
    "        \n",
    "        # entropy (no correction needed)\n",
    "        entropy = (\n",
    "            steerings_dist.entropy() + accels_dist.entropy()\n",
    "        )\n",
    "\n",
    "        steerings = 2*steerings - 1\n",
    "        accels = 2*accels - 1\n",
    "\n",
    "        gases = accels*(accels >= 0)\n",
    "        brakes = -accels*(accels < 0)       \n",
    "\n",
    "        actions = torch.stack([steerings, gases, brakes], dim = 1) # shape: [n_envs, 3]\n",
    " \n",
    "        return (actions, action_log_probs, state_values, entropy, regulation)\n",
    "\n",
    "    def eval_action(\n",
    "        self, x: np.ndarray, actions: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        x = self.PPO_Net.CNN_Net(x)\n",
    "\n",
    "        state_values = self.PPO_Net.Critic_Net(x)\n",
    "        actions_steers_ab, actions_accels_ab = self.PPO_Net.Actor_Net(x)\n",
    "\n",
    "        c_actions_steers = actions_steers_ab[:,0] + actions_steers_ab[:,1]\n",
    "        c_actions_accels = actions_accels_ab[:,0] + actions_accels_ab[:,1]\n",
    "\n",
    "        regulation = F.relu(c_actions_steers - c_max).pow(2) + F.relu(c_actions_accels - c_max).pow(2)\n",
    "\n",
    "        steerings_dist = Beta(actions_steers_ab[:, 0], actions_steers_ab[:, 1])\n",
    "        accels_dist = Beta(actions_accels_ab[:, 0], actions_accels_ab[:, 1])\n",
    "\n",
    "        steerings = (actions[:,0] + 1)/2\n",
    "        accels = (actions[:,1] - actions[:,2] + 1)/2\n",
    "\n",
    "        action_log_probs = (\n",
    "            steerings_dist.log_prob(steerings.clamp(1e-8, 1-1e-8)) + accels_dist.log_prob(accels.clamp(1e-8, 1-1e-8))\n",
    "        )\n",
    "        \n",
    "        entropy = (\n",
    "            steerings_dist.entropy() + accels_dist.entropy()\n",
    "        )\n",
    "\n",
    "        return action_log_probs, state_values, entropy, regulation \n",
    "\n",
    "    def get_advs_returns(\n",
    "        self,\n",
    "        rewards: torch.Tensor,\n",
    "        value_preds: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        masks_d: torch.Tensor,\n",
    "        gamma: float,\n",
    "        lam: float,\n",
    "        active_indices: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        T = len(rewards)\n",
    "        advantages = torch.zeros(T, self.n_envs, device=self.device)\n",
    "\n",
    "        gae = torch.zeros(self.n_envs, device=self.device)\n",
    "        for t in reversed(range(T)):\n",
    "            td_error = (\n",
    "                rewards[t] + gamma*masks[t]*value_preds[t+1] - value_preds[t]\n",
    "            )\n",
    "            gae = td_error + gamma*lam*masks_d[t]*gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "        returns = advantages + value_preds[:-1]\n",
    "\n",
    "        advantages = advantages.flatten().detach()[active_indices]\n",
    "        returns = returns.flatten().detach()[active_indices]\n",
    "        \n",
    "        adv_n = (advantages - advantages.mean())/(advantages.std() + 1e-4).detach()\n",
    "        adv_n = adv_n.clamp(-3.0, 3.0)\n",
    "\n",
    "        return advantages, adv_n, returns\n",
    "        \n",
    "    def get_losses(\n",
    "        self,\n",
    "        action_log_probs: torch.Tensor,\n",
    "        active_indices: torch.Tensor,\n",
    "        ent_coef: float,\n",
    "        reg_coef: float,\n",
    "        states: np.ndarray,\n",
    "        actions: torch.Tensor,\n",
    "        adv_n: torch.Tensor,\n",
    "        returns: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        states = states.reshape(-1, 96, 96, 3)                                #(n_step_per_update*n_env, 96, 96, 3)\n",
    "        actions = actions.reshape(-1, 3).detach()                             #(n_step_per_update*n_env, 3)\n",
    "        action_log_probs_old = action_log_probs.flatten().detach()            #(n_step_per_update*n_env, )   \n",
    "                                                      \n",
    "        action_log_probs_new, state_values, entropy, regulation = self.eval_action(states, actions)           #(n_step_per_update*n_env, )\n",
    "\n",
    "        critic_loss = F.smooth_l1_loss(state_values[active_indices], returns.detach(), beta = 1.0)\n",
    "        #critic_loss = (state_values[active_indices]-returns.detach()).pow(2).mean()\n",
    "\n",
    "        adv_n = adv_n.detach()\n",
    "        \n",
    "        ratio = torch.exp((action_log_probs_new - action_log_probs_old)[active_indices])\n",
    "        surr1 = ratio*adv_n\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * adv_n\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        entropy_loss = - ent_coef*entropy[active_indices].mean()\n",
    "        regulation_loss = reg_coef*regulation[active_indices].mean()\n",
    "        \n",
    "        return critic_loss, actor_loss, entropy_loss, regulation_loss\n",
    "\n",
    "    def update_parameters(\n",
    "        self, \n",
    "        n_iter,\n",
    "        rewards: torch.Tensor,\n",
    "        value_preds: torch.Tensor,\n",
    "        action_log_probs: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        masks_d: torch.Tensor,\n",
    "        active_indices: torch.Tensor,\n",
    "        gamma: float,\n",
    "        lam: float,\n",
    "        ent_coef: float,\n",
    "        reg_coef: float,\n",
    "        states: np.array,\n",
    "        actions: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        advantages, adv_n, returns = self.get_advs_returns(rewards, value_preds, masks, masks_d, gamma, lam, active_indices)\n",
    "\n",
    "        critic_loss_r = 0\n",
    "        policy_loss_r = 0\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "\n",
    "            critic_loss, actor_loss, entropy_loss, regulation_loss = self.get_losses(action_log_probs, active_indices, ent_coef, reg_coef,\n",
    "                                                                                     states, actions, adv_n, returns)\n",
    "    \n",
    "            total_loss = cv*critic_loss + actor_loss + entropy_loss + regulation_loss\n",
    "    \n",
    "            self.PPO_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.PPO_Net.parameters(), max_norm=0.5)\n",
    "            self.PPO_optimizer.step()\n",
    "\n",
    "            critic_loss_r += critic_loss.detach()\n",
    "            policy_loss_r += (actor_loss + entropy_loss + regulation_loss).detach()\n",
    "\n",
    "        return critic_loss_r/n_iter, policy_loss_r/n_iter\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fd731-579b-4153-9f12-08daab430ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment hyperparams\n",
    "\n",
    "n_envs = 8\n",
    "\n",
    "Critic_lr = 1e-3   \n",
    "Actor_lr = 1e-4    \n",
    "CNN_lr = 5e-4     \n",
    "\n",
    "cv = 1\n",
    "n_iter = 4\n",
    "clip_eps = 0.1   \n",
    "\n",
    "reg_coef = 1e-5\n",
    "c_max = 20      \n",
    "\n",
    "\n",
    "base_ent_coef = 0.01 \n",
    "early_step_max = 500\n",
    "mag_ent = 1\n",
    "\n",
    "n_updates = 3000\n",
    "n_steps_per_update = 256\n",
    "randomize_domain = False\n",
    "\n",
    "\n",
    "#agent hyperparams\n",
    "lam = 0.95 # hyper parameter for GAE\n",
    "gamma = 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89ff7b-89b7-4c82-aa50-78877941831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# init the agent\n",
    "\n",
    "agent = PPO(device, n_envs)\n",
    "agent.optimizer(Critic_lr, Actor_lr, CNN_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef094e-a9b8-48ae-8064-49f1b84cb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment setup\n",
    "\n",
    "envs = gym.make_vec(\"CarRacing-v3\", num_envs=n_envs, vectorization_mode=\"async\")\n",
    "\n",
    "states, info = envs.reset(seed=42)\n",
    "\n",
    "rewards_update = []\n",
    "critic_losses = []\n",
    "policy_losses = []\n",
    "entropies = []\n",
    "prog_i = 0\n",
    "\n",
    "skip_nums = np.full((n_envs, ), 50, dtype = np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e84ec-2405-449e-b9b6-948934ab858a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.PPO_Net.train()\n",
    "\n",
    "\n",
    "# use tqdm to get a progress bar for training\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    # we don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    prog_i += 1\n",
    "    \n",
    "    progress = min(prog_i / early_step_max, 1.0)  \n",
    "    mult = mag_ent - (mag_ent - 1.0) * progress         \n",
    "    ent_coef = base_ent_coef * mult\n",
    "\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device = device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device = device)\n",
    "    ep_entropy = torch.zeros(n_steps_per_update, n_envs, device = device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device = device)\n",
    "    masks_d = torch.zeros(n_steps_per_update, n_envs, device = device)\n",
    "    ep_skip_masks = torch.zeros(n_steps_per_update, n_envs, device = device)\n",
    "    ep_states = np.zeros((n_steps_per_update, n_envs, 96, 96, 3))\n",
    "    ep_actions = torch.zeros(n_steps_per_update, n_envs, 3, device = device)\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update+1, n_envs, device = device)\n",
    "        \n",
    "\n",
    "    # play n steps in our parallel environments to collect data\n",
    "    for step in range(n_steps_per_update):\n",
    "\n",
    "        ep_states[step] = states\n",
    "\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        actions, action_log_probs, state_value_preds, entropy, regulation = agent.select_action(\n",
    "            states\n",
    "        )\n",
    "\n",
    "        ep_actions[step] = actions\n",
    "\n",
    "        actions = actions.detach().cpu().numpy()\n",
    "        \n",
    "        active_envs = np.array((skip_nums == 0), dtype = np.int32)\n",
    "        skip_masks = torch.tensor(active_envs, device=device)\n",
    "        ins_inactive_envs = np.where(active_envs == 0)\n",
    "\n",
    "        actions[ins_inactive_envs] = [0.0, 0.0, 0.0]\n",
    "\n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        states, rewards, terminated, truncated, infos = envs.step(\n",
    "            actions\n",
    "        )\n",
    "\n",
    "        ep_value_preds[step] = state_value_preds\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "        ep_entropy[step] = entropy\n",
    "        ep_skip_masks[step] = skip_masks\n",
    "\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "        done = np.array((terminated | truncated), dtype = np.int32)   # (n_envs, )\n",
    "        ids_done = np.where(done == 1)\n",
    "        ins_skip = np.where(skip_nums >= 1)\n",
    "        \n",
    "        skip_nums[ins_skip] -= 1\n",
    "        skip_nums[ids_done] = 50\n",
    "\n",
    "        masks_d[step] = torch.tensor([not term for term in done])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        last_states = agent.PPO_Net.CNN_Net(states)\n",
    "        last_value_preds = agent.PPO_Net.Critic_Net(last_states)\n",
    "\n",
    "    ep_value_preds[-1] = last_value_preds\n",
    "\n",
    "    active_indices = torch.where(ep_skip_masks.flatten() == 1)[0]\n",
    "    \n",
    "        \n",
    "    # update the actor and critic network\n",
    "    critic_loss, policy_loss = agent.update_parameters(\n",
    "        n_iter,\n",
    "        ep_rewards,\n",
    "        ep_value_preds,\n",
    "        ep_action_log_probs,\n",
    "        masks,\n",
    "        masks_d,\n",
    "        active_indices,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        reg_coef,\n",
    "        ep_states,\n",
    "        ep_actions,\n",
    "    )\n",
    "    \n",
    "    # Log the losses and entropy\n",
    "    rewards_update.append(np.mean(ep_rewards.detach().cpu().numpy()))\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    policy_losses.append(policy_loss.detach().cpu().numpy())\n",
    "    entropies.append(ep_entropy.detach().mean().cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e86dfa-8779-498c-b2ef-f41d93ed2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "rolling_length = 50\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the CarRacing-v3 environment \\n \\\n",
    "    (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={randomize_domain})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0].set_title(\"Rewards\")\n",
    "rewards_update_moving_average = (\n",
    "    np.convolve(np.array(rewards_update), np.ones(rolling_length), mode=\"valide\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(rewards_update_moving_average)\n",
    "axs[0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# entropy\n",
    "axs[1].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valide\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(entropy_moving_average)\n",
    "axs[1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "# critic loss\n",
    "axs[2].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    /rolling_length\n",
    ")\n",
    "axs[2].plot(critic_losses_moving_average)\n",
    "axs[2].set_xlabel(\"Number of updates\")\n",
    "#axs[2].set_yscale('log')\n",
    "\n",
    "# actor loss\n",
    "axs[3].set_title(\"policy Loss\")\n",
    "policy_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(policy_losses), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    /rolling_length\n",
    ")\n",
    "axs[3].plot(policy_losses_moving_average)\n",
    "axs[3].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046d256-8f83-4f4e-9c08-4f609e275264",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights = True\n",
    "load_weights = False\n",
    "\n",
    "PPO_weights_path = \"weights/PPO_weights_reward_0.9.hS\"\n",
    "\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.mkdir(\"weights\")\n",
    "\n",
    "\"\"\" save network weights \"\"\"\n",
    "if save_weights:\n",
    "    torch.save(agent.PPO_Net.state_dict(), PPO_weights_path)\n",
    "\n",
    "\"\"\" Load network weights \"\"\"\n",
    "\n",
    "if load_weights:\n",
    "    agent.PPO_Net.load_state_dict(torch.load(PPO_weights_path))\n",
    "    agent.PPO_Net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfc43b-f81d-4ee9-9058-c1e40ec172bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" play a showcase episode \"\"\"\n",
    "\n",
    "video_folder = \"video\"\n",
    "\n",
    "if not os.path.exists(video_folder):\n",
    "    os.mkdir(video_folder)\n",
    "\n",
    "env_v = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array_list\")\n",
    "\n",
    "agent.PPO_Net.eval()\n",
    "\n",
    "# get an initial state\n",
    "state_v, info_v = env_v.reset()\n",
    "for i in range(50):\n",
    "    state_v, reward_v, terminated_v, truncated_v, info_v = env_v.step(np.array([0.0, 0.0, 0.0]))\n",
    "\n",
    "# play one episode\n",
    "done_v = False\n",
    "while not done_v:\n",
    "    # select an action A_{t} using S_{t} as input for the agent\n",
    "    with torch.no_grad():\n",
    "        action_v,_,_,_,_ = agent.select_action(state_v[None,:])\n",
    "\n",
    "    # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "    state_v, reward_v, terminated_v, truncated_v, info_v = env_v.step(action_v.detach().cpu().numpy().squeeze())\n",
    "\n",
    "    # update if the environment is done\n",
    "    done_v = terminated_v or truncated_v\n",
    "\n",
    "# make a video\n",
    "save_video(frames=env_v.render(), video_folder=video_folder, fps=env_v.metadata[\"render_fps\"])\n",
    "\n",
    "env_v.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
